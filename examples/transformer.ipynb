{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c56226d9-be44-4db2-ae09-04f355dced98",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "This notebook trains a decoder-only transformer to perform next token prediction on the Tiny Shakespeare dataset.\n",
    "\n",
    "The parameters are chosen to make training fast (e.g., short context) and will not produce a performant model.\n",
    "The interested reader should feel free to alter these to produce a larger model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31c06100-f075-403d-91e2-60bc4bd142a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import micrograd_pp as mpp\n",
    "import numpy.typing as npt\n",
    "\n",
    "np = mpp.numpy  # `import numpy as np` will not work when using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e15b90d-c80a-4bc5-8cca-f2fd4655fb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_WIDTH = 32\n",
    "DROPOUT = 0.0\n",
    "EMBEDDING_DIM = 128\n",
    "EVAL_FREQ = 3_000\n",
    "HIDDEN_SIZE = EMBEDDING_DIM * 4\n",
    "LEARNING_RATE = 0.1\n",
    "NUM_BLOCKS = 3\n",
    "NUM_HEADS = 4\n",
    "NUM_ITERS = 30_000\n",
    "TRAIN_BATCH_SIZE = 96\n",
    "TRAIN_FRAC = 0.99\n",
    "VAL_BATCH_SIZE = 4_096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e23ae66-1627-438f-8de7-d09524bf6b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = mpp.datasets.load_tiny_shakespeare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "288a0a7d-c6ef-4fd8-8716-c390bc0ca3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "char2token = {char: token for token, char in enumerate(vocab)}\n",
    "all_tokens = np.array([char2token[char] for char in text], dtype=np.int32)\n",
    "\n",
    "first_val_index = int(TRAIN_FRAC * all_tokens.size)\n",
    "train_tokens = all_tokens[:first_val_index]\n",
    "val_tokens = all_tokens[first_val_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b48de65-5e4c-4b2d-a5f5-55c3b0811fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block:\n",
    "    def __init__(self) -> None:\n",
    "        self._ln1 = mpp.LayerNorm(EMBEDDING_DIM)\n",
    "        self._attn = mpp.MultiheadAttention(\n",
    "            embed_dim=EMBEDDING_DIM,\n",
    "            num_heads=NUM_HEADS,\n",
    "            batch_first=True\n",
    "        )\n",
    "        attn_mask_np = np.zeros((CONTEXT_WIDTH, CONTEXT_WIDTH))\n",
    "        attn_mask_np[np.triu_indices_from(attn_mask_np, k=1)] = -np.inf\n",
    "        self._attn_mask = mpp.Constant(attn_mask_np)\n",
    "        self._dropout = mpp.Dropout(DROPOUT)\n",
    "        self._ln2 = mpp.LayerNorm(EMBEDDING_DIM)\n",
    "        self._ff = mpp.Sequential(\n",
    "            mpp.Linear(in_features=EMBEDDING_DIM, out_features=HIDDEN_SIZE),\n",
    "            mpp.ReLU(),\n",
    "            mpp.Linear(in_features=HIDDEN_SIZE, out_features=EMBEDDING_DIM),\n",
    "            mpp.Dropout(DROPOUT),\n",
    "        )\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: mpp.Expr  # (N, L, E)\n",
    "    ) -> mpp.Expr:\n",
    "        x = self._ln1(x)\n",
    "        x = x + self._dropout(self._attn(x, x, x, attn_mask=self._attn_mask)[0])\n",
    "        x = self._ln2(x)\n",
    "        x = x + self._ff(x)\n",
    "        return x  # (N, L, E)\n",
    "\n",
    "class DecoderOnlyTransformer:\n",
    "    def __init__(self) -> None:\n",
    "        self._tok_embedding = mpp.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            label=\"token_embedding\",\n",
    "        )\n",
    "        self._pos_embedding = mpp.Embedding(\n",
    "            num_embeddings=CONTEXT_WIDTH,\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            label=\"positional_embedding\",\n",
    "        )\n",
    "        self._blocks = mpp.Sequential(*[Block() for _ in range(NUM_BLOCKS)])\n",
    "        self._ln = mpp.LayerNorm(EMBEDDING_DIM)\n",
    "        self._output_proj = mpp.Linear(\n",
    "            in_features=EMBEDDING_DIM,\n",
    "            out_features=vocab_size,\n",
    "            label=\"output_projection\",\n",
    "        )\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        tokens: npt.NDArray,  # (N, L)\n",
    "    ) -> None:\n",
    "        t = self._tok_embedding(tokens)  # (N, L, E)\n",
    "        p = self._pos_embedding(np.arange(CONTEXT_WIDTH))  # (L, E)\n",
    "        x = t + p  # (N, L, E)\n",
    "        x = self._blocks(x)  # (N, L, E)\n",
    "        x = self._ln(x)  # (N, L, E)\n",
    "        return self._output_proj(x)  # (N, L, V)\n",
    "\n",
    "def loss(model: mpp.Module, indices: npt.NDArray, user_data: npt.NDArray) -> mpp.Expr:\n",
    "    \"\"\"Compute loss on a batch.\"\"\"\n",
    "    x = np.stack([user_data[index - CONTEXT_WIDTH    :index    ] for index in indices])  # (N, L)\n",
    "    y = np.stack([user_data[index - CONTEXT_WIDTH + 1:index + 1] for index in indices])  # (N, L)\n",
    "    yhat = model(x).reshape((-1, vocab_size))  # (N * L, V)\n",
    "    y = y.reshape(-1)  # (N * L,)\n",
    "    return mpp.cross_entropy_loss(yhat, y)\n",
    "\n",
    "def train_loss(model: mpp.Module) -> mpp.Expr:\n",
    "    \"\"\"Compute loss on a random batch from the training set.\"\"\"\n",
    "    indices = np.random.randint(low=CONTEXT_WIDTH, high=train_tokens.size, size=(TRAIN_BATCH_SIZE,))\n",
    "    return loss(model=model, indices=indices, user_data=train_tokens)\n",
    "\n",
    "def val_loss(model: mpp.Module) -> npt.NDArray:\n",
    "    \"\"\"Approximate loss on the validation set.\"\"\"\n",
    "    losses = []\n",
    "    with mpp.eval(), mpp.no_grad():\n",
    "        n = 0\n",
    "        low = CONTEXT_WIDTH\n",
    "        while low < val_tokens.size:\n",
    "            high = min(low + VAL_BATCH_SIZE, val_tokens.size)\n",
    "            indices = np.arange(low, high)\n",
    "            item = loss(model=model, indices=indices, user_data=val_tokens).value\n",
    "            losses.append(item)\n",
    "            low = high\n",
    "        return np.array(losses).mean()\n",
    "\n",
    "def generate_sentence(model: mpp.Module, init: npt.NDArray | None = None, length: int = 512) -> str:\n",
    "    \"\"\"Use a learned decoder-only transformer to generate a sentence.\"\"\"\n",
    "    with mpp.eval(), mpp.no_grad():\n",
    "        if init is None:\n",
    "            init = np.zeros((CONTEXT_WIDTH,), dtype=np.int32)\n",
    "        context = init\n",
    "        tokens = []\n",
    "        for _ in range(length):\n",
    "            logits = model(context.reshape(1, -1))\n",
    "            pvals = mpp.softmax(logits, dim=-1)[0, -1, :]\n",
    "            token = np.random.multinomial(n=1, pvals=pvals.value).argmax().item()\n",
    "            context[:-1] = context[1:]\n",
    "            context[-1] = token\n",
    "            tokens.append(token)\n",
    "        return ''.join(vocab[token] for token in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78207a31-d5a7-4fc3-b86f-ee93b6804bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uninitialized Embedding\n",
      "-----------------------\n",
      "Loss: 4.616665830144252\n",
      "Random sentence: wWm\n",
      "!YuG.b3xgXgfuNcXiw,PGmOW,xnxg.xgufo,xZTfiwLmXlpLWnm.g\n",
      "gQxBzX,\n",
      "!''c,cTBwgH,YfYw,?.gw.m'w'uYT,XRgmt nXX\n",
      "kIhLD\n",
      "EHr?JgOXC,!mztx,,,RakcYlZuXc,G?ZozU?wgu'z,?wg\n",
      "z\n",
      "Zg.'PXNT?xgN'XKiwwxmR,gJD,gGs Xbfw&R,XG.q.lfz,,\n",
      "o,u'wWZLLzFSXGmnUTSw\n",
      "LXXLLnzBcaiwXRguuFcggYX,;'ohcLR cpcYQx,gg&!IRiR,XsxxLwwh,tgwgHmkjYu\n",
      "Gc,XbXB,ifxBuG,mpfg$&xm,g?RULmJlwU'XB'XbLwwjfB\n",
      ",Wug&uug\n",
      "B,LcX&N&XX\n",
      "ByX,l! \n",
      "k\n",
      "D?Vr,l?HP,HcZzR?fu,iXYuufwx,tac,gDitN$!!m\n",
      "uKtDqhm.ZR\n",
      "LmTfx,bu''bf\n",
      "O\n",
      "ufOKGb-Ykm,m!W,Bu,L TcotB X'wccuaigl,$WgLBwBbfRXYlBJ&WmZugvXF&KXX?G\n",
      ".z\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "model = DecoderOnlyTransformer()\n",
    "\n",
    "print(f\"\"\"\n",
    "Uninitialized Embedding\n",
    "-----------------------\n",
    "Loss: {val_loss(model).item()}\n",
    "Random sentence: {generate_sentence(model)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64662026-ea81-4c9e-aede-0dbf484424a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration        0\n",
      "------------------\n",
      "Loss: 4.616665830144252\n",
      "Random sentence: wG'swRgmGRDiwGYwmalYu g?BYcX\n",
      "XlixXx'zJmXc.,I-MuUmuXgYmzN,xLgqNu.wl,cXm,?mGK,uK&Xbo,fRzXckuWXBfBuK,K,3wgguGtG$mZx,uLgex\n",
      "'oK',kW\n",
      "Mcm\n",
      "u!fwu.muabuDhxgR'fzYlNDXP\n",
      "guX,xG,BQ,Rdf,YXV,cTg'B,$x,zv.\n",
      "wK$wz&BuldX$fWg\n",
      ",KBfBuYli\n",
      "G $mXguu,NiX,uTBwLmu:B.ww$vwS,mXxuB,WRDViRiDGlVPt-XmG,'vTBR,NKgm'Z\n",
      "ZbumLXKw,?D,,mGLz yhqHjwmx,eG3$bStFRZtXlRi3fdfumUP'cVFg,BK&iKiLlxgqko,OTx,ntq,RKXYufVknlKn,MVyO'O'w-mZTKiLxB Yu'UIUBfQuwwWK.oG $iczmufU'wrG',xBKivwmRBilCjFjhBPwePuWg&3,Ml$l,mxg\n",
      "Nhwov,gLg$B;Duc&X'CHwwtG!,DitXDt.w-aoRHXgge,nzVDXP&uUw\n",
      "\n",
      "\n",
      "Iteration     3000\n",
      "------------------\n",
      "Loss: 2.0463488263845897\n",
      "Random sentence: CRUMINAMELELVOCENTA:\n",
      "Our I hangir, shall waing I befiste it unnt: en not hem ben starome not to for elier plese,\n",
      "If bese, I'n cupasely tend ald thie, patiledn.\n",
      "Pear at wit Fram wh me som kinequess. I bun stou!\n",
      "Sin to that morrie met the was heasol un Ve,\n",
      "Gry, Ratings:\n",
      "'Sels for, sickinch, baddled kith\n",
      "Tild bruck bemp-manes the trutist\n",
      "I then not not semeren; thou wart hald in eward my yether.\n",
      "Wath morreathem canstere, Decupe as a holy!\n",
      "This woman you werver so's pon doun ton wheme,\n",
      "Our the thereer then yren\n",
      "\n",
      "\n",
      "Iteration     6000\n",
      "------------------\n",
      "Loss: 1.8789769487645624\n",
      "Random sentence: TRANUS:HORY IS:\n",
      "So sunir' he\n",
      "no? Yello boy, the sear 'ermblows hath nough'd;\n",
      "Foir,\n",
      "But sir, and four uncling lew; thou arm such\n",
      "And crade the sestfest isure, I ass he conters he looke of mouch Edward your are comps.\n",
      "\n",
      "MENEN:\n",
      "Haure all becound you come, promout boss.\n",
      "Ge Edward withichs thank not my\n",
      "Bout: thought Salong and when Whickty he have capile of Sild, it the our of besen.\n",
      "\n",
      "MONTA:\n",
      "\n",
      "SLIEXS:\n",
      "Yet th thee being old cousin, like so be this worse:\n",
      "Inas evers of mine.\n",
      "\n",
      "QUEEN MENIO:\n",
      "I'll not?\n",
      "\n",
      "KING EDWICHABRD:\n",
      "\n",
      "\n",
      "Iteration     9000\n",
      "------------------\n",
      "Loss: 1.7874336620118776\n",
      "Random sentence: LEONCEES:\n",
      "OFider, my lord, I forsaid not.\n",
      "Provost, gral ade heaven busine sorring of like our so life! slate.\n",
      "\n",
      "MINARENBIUS:\n",
      "What will me?\n",
      "\n",
      "KING RICATHAND:\n",
      "Come, in think your battle\n",
      "Richord citlence of findrise earge andry's body and cruded friend, I head,\n",
      "'Tis sea u' say say their nor\n",
      "five my missables brow my etchments\n",
      "That devish mader himself womed caris afform\n",
      "Then blood, no there, that dukes hare in have that shalt to you hurselved like on stup him have floth lius him his buriness,\n",
      "That every nament s\n",
      "\n",
      "\n",
      "Iteration    12000\n",
      "------------------\n",
      "Loss: 1.7266077093835357\n",
      "Random sentence: ANGELLOUGHARD III:\n",
      "Mision confall'd\n",
      "merch'd to baxe\n",
      "Hus cease ears a choled friends,\n",
      "And sighdoney, for my out light\n",
      "Of fields, a moure as ill will beer leave,\n",
      "But prove see of a shack, 'That the be's that may his break cer words your hours' is his will Frane,--\n",
      "And not that He lies, woe swill had his rese are slaight:\n",
      "Be, I very condey are is guest.\n",
      "\n",
      "ISABELLA:\n",
      "All lenemorshed more day, my doth in me,\n",
      "Were upon'd of Nuner brearing of case one reson fwars. Frow't of Lord Haw be our vengean\n",
      "While that you are\n",
      "\n",
      "\n",
      "Iteration    15000\n",
      "------------------\n",
      "Loss: 1.6730787704937757\n",
      "Random sentence: PELVER:\n",
      "In Kathark'd. HERgonvy I have ancient\n",
      "Then wichardles them to thee? kin he ome,\n",
      "By sates too? or sincle. Then hands.\n",
      "\n",
      "LORD HENRY VI:\n",
      "A think of Prince own this?\n",
      "\n",
      "Provost:\n",
      "Yet,\n",
      "Come, the suppon th me other once first.\n",
      "\n",
      "CATESBY:\n",
      "The kind it, if you, not fit is my sheeth:\n",
      "Welcomes it.\n",
      "\n",
      "BUCKINGHAM:\n",
      "But, Thou dost thou half--\n",
      "For your fot hear ewell ye\n",
      "The it not soul be She home? marks, and\n",
      "Mayor, master, and neither furning of woe\n",
      "Of my foot. Angelo.\n",
      "\n",
      "CLARENCE:\n",
      "I will must seasoes, if thou hads!\n",
      "Broke,\n",
      "\n",
      "\n",
      "Iteration    18000\n",
      "------------------\n",
      "Loss: 1.660302514208628\n",
      "Random sentence: KATHARINA:\n",
      "It is't the king nor in Juliet? say,\n",
      "'Who bold the with a gentle me.\n",
      "\n",
      "ROMEO:\n",
      "I for my queen's heir coward, cousin,\n",
      "you honesty, you lie tree to meet,\n",
      "We broke thy ham; old God, sir, you say't,\n",
      "help, yet him upon the account more\n",
      "Is with answer's cry your annerby,\n",
      "For this my call their bar their sit live. say, prifit\n",
      "the cease you?\n",
      "\n",
      "Third Citizen:\n",
      "Be with is not thee; fear, then, bite prove said the court,\n",
      "But come soul-naw, and awry arm!\n",
      "He a good eit! O emprected ours,\n",
      "That you? defent not agiv\n",
      "\n",
      "\n",
      "Iteration    21000\n",
      "------------------\n",
      "Loss: 1.6212410996272009\n",
      "Random sentence: IVIRCLIUS:\n",
      "What, that show to in min.\n",
      "\n",
      "KING RICHARD II:\n",
      "Hereford I.\n",
      "\n",
      "VOLUMNIAD:\n",
      "This viold year never it, having,\n",
      "\n",
      "ISABELLA:\n",
      "Dear intershal. What's tenters, underfection\n",
      "To sundance, men, to choins wall paracus.\n",
      "May you depent her hand my more both\n",
      "With aford her cament and sweeter.\n",
      "\n",
      "CATESBY:\n",
      "Thy know not to-mood.\n",
      "\n",
      "KING HENRY VI:\n",
      "O mind spents what my lord.\n",
      "\n",
      "Second Mercy:\n",
      "Henry returned to then that do back child.\n",
      "\n",
      "First Lord:\n",
      "I possess'd deal.\n",
      "\n",
      "DUKE OF YORK:\n",
      "Now to my sent good daughter receiveth all on my\n",
      "\n",
      "\n",
      "Iteration    24000\n",
      "------------------\n",
      "Loss: 1.6104857896807314\n",
      "Random sentence: SOMELLIA:\n",
      "Prithat, sir, sir, ay, they dispatch undoin's say'st her.\n",
      "\n",
      "KING RICHARD III:\n",
      "It wot not lets thyself! for what, night.\n",
      "\n",
      "TRANIO:\n",
      "He is you have well; what nature the and be broke;\n",
      "Made you field'd, so honour purpose,\n",
      "So think in my tongue heart-\n",
      "Follow the vaintain which you tiestal affects: now lever for his\n",
      "A vilet shall pickly to your losh:\n",
      "What that wont them so brooke; but give your away,\n",
      "Comford, his the fiery: that winters!\n",
      "Com on, and she and he nature?\n",
      "\n",
      "BRUTUS:\n",
      "Chone unto this hastest, and\n",
      "\n",
      "\n",
      "Iteration    27000\n",
      "------------------\n",
      "Loss: 1.5970872732121075\n",
      "Random sentence: MENENIUS:\n",
      "You this, be bed wither, speak me a foot\n",
      "with ustart, lord, Prithee do me to have thes, they say news but\n",
      "Upon aboy, if than their chield with gistence earth than heard that man are unjustiful,\n",
      "Dispat him and right, friends, hell your the Ratch,\n",
      "And tears framed to see to an our sork,\n",
      "When hath was, unry from me. Lady antigation York!\n",
      "These that cheers that oath\n",
      "To fly me erbody, what, that he them,\n",
      "Under was no mine rock sog so with the hand?\n",
      "\n",
      "DUKE OF AUMERLE:\n",
      "Nay, lords, mark'd, Camillo my now,\n",
      "\n",
      "\n",
      "\n",
      "Iteration    30000\n",
      "------------------\n",
      "Loss: 1.587523492528424\n",
      "Random sentence: DUREGORE:\n",
      "Threeches nothing wosts, and I cannot stones: I repart of withest but upon throat,\n",
      "And makes for WARWICK:\n",
      "Gram forth of his aunt\n",
      "have his horse; I'll am I common block more than a grieves and sworn in the head of lady?\n",
      "\n",
      "ISABELLA:\n",
      "How my sope of Milate full even such as leave my officers.\n",
      "\n",
      "GLOUCESTER:\n",
      "Is woo although hath we are to deparch your chanced to urge your honour: morrow, unpiscase of other\n",
      "Which ship ranches followers?\n",
      "\n",
      "Propostment:\n",
      "Sent those inforcement.\n",
      "\n",
      "Teglimman:\n",
      "He common harm of th\n",
      "\n"
     ]
    }
   ],
   "source": [
    "opt = mpp.SGD(lr=LEARNING_RATE)\n",
    "\n",
    "n = 0\n",
    "while True:\n",
    "    if n % EVAL_FREQ == 0:\n",
    "        print(f\"\"\"\n",
    "Iteration {n:8d}\n",
    "------------------\n",
    "Loss: {val_loss(model).item()}\n",
    "Random sentence: {generate_sentence(model)}\n",
    "\"\"\")\n",
    "\n",
    "    if n >= NUM_ITERS:\n",
    "        break\n",
    "\n",
    "    train_loss(model).backward(opt=opt)\n",
    "\n",
    "    n += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
